---
title: "" #ECL750<br>François Rousseu<br>Hiver 2021
author: ""
date: ""
output:
  html_document:
    depth: 5
    fig_height: 5
    fig_width: 6
    number_sections: no
    toc: yes
    toc_depth: 5
    toc_float:
      collapsed: yes
      smooth_scroll: no
editor_options: 
  chunk_output_type: console
---

<script>
    $(document).ready(function() {
      $items = $('div#TOC li');
      $items.each(function(idx) {
        num_ul = $(this).parentsUntil('#TOC').length;
        $(this).css({'text-indent': num_ul * 20, 'padding-left': 0});
      });

    });
</script>


<style>

pre.r {
    background-color: #EEEEEE;
    border-color: #DDDDDD;
    font-size: 12pt;
}

pre code {
  font-size: 12pt;
}

body {
  font-size: 14pt;
}

.main-container {
    max-width: 1700px !important;
}

#TOC {
  font-size: 8pt;
  border-color: white;
  max-width: 12vw;
}

.list-group-item.active:focus{
    z-index: 2;
    color: darkgreen;
    background-color: #EEEEEE;
    border-color: red;
    font-weight: bolder;
    font-color: red;
}

.list-group-item.active:hover {
    z-index: 2;
    color: darkgreen;
    background-color: #EEEEEE;
    border-color: red;
    font-weight: bolder;
    font-color: red;
}

.list-group-item.active{
    z-index: 2;
    color: darkgreen;
    background-color: #EEEEEE;
    border-color: red;
    font-weight: bolder;
    font-color: red;
}

h1.title {
  margin-top: 50px;
  font-size: 42px;
  color: DarkGreen;
  font-weight: bold;
}

h1, h2, h3, h4, h5, h6 {
  margin-top: 5vh;
  padding-top: 20px;
  padding-bottom: 50px;
  color: DarkGreen;
  font-weight: bold;
  font-size: 25px;
}

.pres { /* write {.pres} next to each section to apply css  */
   padding-top: 100px;
   margin-bottom: 700px;
}

</style>

```{r setup, include=FALSE}
library(scales)
knitr::opts_chunk$set(echo=TRUE, tidy=TRUE, error=TRUE, fig.align="center", fig.width=8, fig.height=6, cache=FALSE)
options(width=150)
```

<br>


# 1. Stats de base, modèles linéaires, GLM


## Distribution statistique

Décrit les probabilités d'apparition des valeurs d'une distribution donnée.


### Distribution normale

On parle aussi souvent d'une distribution Gaussienne.

<br>
  
$$ f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2} $$

<br>

où $\mu$ et $\sigma$ sont des paramètres, respectivement la moyenne et l'écart-type. 


#### Forme de la distribution normale

```{r}

x<-seq(-5,15,by=0.01)
mu<-5
sigma<-2
y<-(1/(sigma*sqrt(2*pi))*exp((-1/2)*((x-mu)/sigma)^2))
plot(x,y,type="l")

```

#### Simuler une distribution normale

On peut utiliser la fonction `rnorm` pour simuler des valeurs provenant d'une distribution normale.

```{r}
set.seed(123)
n<-1000
x<-rnorm(n,mu,sigma)
hist(x,breaks=50)
```

#### Interprétation

```{r}

v<-seq(-5,15,by=0.01)
hist(x,breaks=50,freq=FALSE,xlim=range(v))
curve(dnorm(x,mu,sigma),add=TRUE)
y<-dnorm(v,mu,sigma)
polygon(c(v[v>=6 & v<=8],8,6), c(y[v>=6 & v<=8],0,0),col=alpha("red",0.5),border=NA)

```

L'aire sous la courbe est de 1 et la proportion de la surface en rouge représente la probabilité d'obtenir une valeur entre 6 et 8 avec une moyenne de $\mu = 5$ et un écart-type de $\sigma = 2$. On peut estimer cette probabilité avec la fonction `pnorm` intégrée à R. Ce type de fonction est intégré pour plusieurs distribution statistique (voir `?Distributions`).


##### Probabilité d'obtenir une valeur? 

```{r}

pnorm(5,mean=5,sd=2) # probabilité d'obtenir une valeur <= 5
pnorm(-2,mean=5,sd=2) # probabilité d'obtenir une valeur <= -2
pnorm(4,mean=5,sd=2) # probabilité d'obtenir une valeur <= 4

```

Probabilité d'obtenir une valeur entre 6 et 8

```{r}

pnorm(8,5,2)-pnorm(6,5,2)

```


#### Distribution d'une variable

<br>

$$ X \sim \mathcal{N}(\mu,\,\sigma^{2})\,$$

<br>

$X$ suit une distribution normale avec une moyenne $\mu$ et une variance $\sigma^{2}$
 

## Erreur-type

<br>

$$ se = \frac{sd}{\sqrt{n}} $$

<br>

ou

<br>

$$ \sigma_{\bar{x}} = \frac{\sigma_{x}}{\sqrt{n}} $$

<br>

```{r}
n<-100000
pop<-rnorm(n,100,20) # population fictive de n individus avec une moyenne de 100 et écart-type de 20
ech<-sample(pop,30) # échantillon aléatoire de 30 individus
mean(ech) # moyenne
sd(ech)/sqrt(30) # erreur-type
```


### Simulation

Ici, on refait cet échantillonnage `nreps` fois et on calcule l'écart-type des différentes moyennes obtenues.

```{r}

nreps<-1000
ech1000<-sapply(1:nreps,function(i){
  s<-sample(pop,30)	
  mean(s)
})
hist(ech1000)
```

#### Écart-type des valeurs simulées

```{r}

sd(ech1000)

```

Comparons ceci à l'erreur-type calculé à partir de notre seul échantillon:

```{r}

sd(ech)/sqrt(30) # erreur-type

```

##### Interprétation de l'erreur-type

L'écart-type de ces moyennes correspond à l'erreur-type estimée à partir d'un seul échantillon. En d'autres mots, l'erreur-type représente:


* l'écart-type si des moyennes obtenues si on refaisait plusieurs fois le même échantillonnage

* une mesure de précision dans l'estimation de  la moyenne

<br>

Le concept d'erreur-type s'applique aussi à d'autres paramètres estimés et non seulement à la moyenne.  

<br>

L'erreur-type d'une statistique ou d'un paramètre est une estimation de l'écart-type de sa distribution d'échantillonnage.


## Intervalle de confiance

Reprenons notre population fictive `pop` qui a une moyenne de 100 et un écart-type de 20. On se rappelle qu'un intervalle de confiance pour une moyenne provenant d'un échantillon distribué normalement (ou presque) se calcule de la façon suivante:

<br>

$$\bar{x} ± t_{dl,\alpha/2}\sigma_{\bar{x}}$$

<br>

On peut calculer cet intervalle de confiance avec R:

```{r}

mean(ech)+qt(0.025,df=length(ech)-1,lower.tail=FALSE)*(sd(ech)/sqrt(length(ech)))*c(-1,1)

```

### Simulation

Maintenant, reprenons note population fictive et calculons à chaque fois cet intervalle de confiance. Ensuite, calculons quelle est la proportion des intervalles qui contiennent la véritable moyenne de 100.

```{r}

nreps<-1000
ech1000<-lapply(1:nreps,function(i){
  s<-sample(pop,30)	
  c(mean(s),mean(s)+qt(0.025,df=length(s)-1,lower.tail=FALSE)*(sd(s)/sqrt(length(s)))*c(-1,1))
})
ci<-as.data.frame(do.call("rbind",ech1000))
names(ci)<-c("mean","lowerCI","upperCI")

sum(100>=ci$lowerCI & 100<=ci$upperCI)/nreps

```

### Interprétation de l'intervalle de confiance

Un intervalle de confiance à 95% veut dire que 95% des intervalles de confiance qu'on obtiendrait si on refaisait le même échantillonnage contiendront la véritable moyenne de la population.

Plus généralement, on devrait parler d'un paramètre (moyenne, variance, pente, etc.) plutôt que d'une moyenne.

On entend souvent à tort que cela veut dire qu'il y a 95% de chances que le paramètre soit à l'intérieur des bornes. Or le paramètre est considéré comme étant fixe et la probabilité porte sur le fait que l'intervalle contienne la véritable valeur du paramètre ou pas.

Tout comme l'erreur-type, l'intervalle de confiance est une mesure de précision dans l'estimation d'un paramètre.


## Valeur de p

Un exemple avec le test de $t$ et une population fictive.
```{r}

set.seed(1)
n<-10000
pop1<-rnorm(n,20,3) # population fictive de n individus avec une moyenne de 20
pop2<-rnorm(n,21,3) # population fictive de n individus avec une moyenne de 21

ech1<-sample(pop1,10) # échantillon de 10 individus de chaque population
ech2<-sample(pop2,10)

test<-t.test(ech1,ech2)
test

```

### Interprétation de la Valeur de p

```{r}

plot(0,0,xlim=c(-4,4),ylim=c(0,0.5),type="n")
curve(dt(x,test$parameter),add=TRUE)
v<-seq(-5,15,by=0.01)
y<-dt(v,test$parameter)
polygon(c(v[v<=test$statistic],test$statistic,min(v)), c(y[v<=test$statistic],0,0),col=alpha("red",0.5),border=NA)
polygon(c(v[v>=abs(test$statistic)],max(v),abs(test$statistic)), c(y[v>=abs(test$statistic)],0,0),col=alpha("red",0.5),border=NA)
abline(v=rep(test$statistic,2)*c(1,-1),lwd=2,lty=3,col="red")



```

### Simulations

Reprenons le test de $t$ précédent, mais cette fois, prenons nos échantillons dans la même population (pop1) à chaque fois.

```{r}

nreps<-1000

pvalue<-sapply(1:nreps,function(i){
  ech1<-sample(pop1,5) 
  ech2<-sample(pop1,5)
  test<-t.test(ech1,ech2)
  test$p.value#statistic
})

hist(pvalue,breaks=50)
abline(v=0.05,col="red",lwd=2,lty=3)
sum(pvalue<=0.05)/length(pvalue)


```


## Modèles linéaires simples

<br>

On a ici la formulation classique ou habituelle d'un modèle linéaire simple.

<br>

$$ y = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} +  ...  + \beta_{n}x_{n} + \epsilon $$

$$ \epsilon \sim \mathcal{N}(0,\,\sigma^{2})\ $$


### Autre formulation

<br>

$$ \mu = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} +  ...  + \beta_{n}x_{n}$$

$$ y \sim \mathcal{N}(\mu,\,\sigma^{2})\ $$

<br>

où 

<br>

$y$ = observations

$\mu$ = prédicteur linéaire

On a donc un modèle représentant le lien entre nos variables et on cherche à estimer les **paramètres** de ce modèles à l'aide de données.

### Suppositions de bases

Quelles sont les suppositions de bases d'un modèle linéaire tel que celui présenté?

* Normalité des résidus? 

* Homoscédasticité de la variance (la variance ne dépend pas de ou ne varie pas avec $x_{n}$)

* Linéarité des relations entre $x_{n}$ et $y$?

* Corrélation entre les $x_{n}$?

* Indépendence entre les observations?

<!-- peut-êwtre compléter avec une slide sur chaque et des solutions -->

### Simulations

* Une des meilleures façons de s'assurer qu'on comprend comment fonctionne un modèle

* Permet d'étudier le comportement d'un modèle avec des valeurs connues


### Simulons un modèle

```{r}
# nb d'observations
n<-75

# valeurs fictives des variables explicatives selon une distribution uniforme
x1<-runif(n,0,100)
x2<-runif(n,0,10)
x3<-runif(n,0,1)

# valeurs des paramètres beta
b0<-100
b1<-0.2
b2<-1
b3<--10

# prédicteur linéaire
mu<-b0+b1*x1+b2*x2+b3*x3

# observations à partir d'une distribution normale avec un écart-type de 5
y<-rnorm(n,mu,5)

# données
d<-data.frame(y,x1,x2,x3)

```

### Fonction `lm`

La fonction `lm` permet de faire des modèles linéaires simples, ce qui inclue les régressions simples, les ANOVAs, ANCOVAs, régression multiples, etc.

Remarquez l'utilisation de l'argument `data` où la fonction ira chercher les différents éléments spécifiés dans la formule.

```{r}

m<-lm(y~x1+x2+x3,data=d)

```

#### Coefficients

On peut appliquer plusieurs fonctions pour extraire les éléments d'un modèle, en particulier un modèle effectué avec la fonction `lm`.

```{r}
coef(m)
```

#### Sommaire

La fonction `summary` est la plus utile pour extraire l'information d'un modèle.

```{r}
summary(m)
```

<!--

##### Résidus

```{r}
summary(m)
```

<br>

La section `Residuals:` illustre la distribution des valeurs des résidus tout en illustrant les valeurs les plus extrêmes.

##### Coefficients

```{r}
summary(m)
```

<br>

Ici, la section `Coefficients:` donne chacun des paramètres associés au modèle. Pour chaque coefficient, un test est effectué et pour chaque test l'hypothèse nulle est que le coefficient est égal à zéro ($H_{0}: \beta_{p} = 0$). Remarquez que les coefficients suivent une distribution du t de Student. La façon de représenter la valeur de p représente bien le fait que cette valeur représente une probabilité d'obtenir un telle valeur de t donnée ou pire encore.

##### R2

```{r}
summary(m)
```

<br>

`Multiple R-squared` est le coefficient de détermination ($R^{2}$). Il représente la proportion de variance expliquée par les variables explicatives (i.e. par le modèle).

<br>

$$R^{2} = \frac{\textrm{Variance expliquée}}{\textrm{Variance totale}}$$

<br>

`Adjusted R-squared` est le coefficient de détermination ajusté pour le nombre de paramètres dans le modèle. Contrairement au $R^{2}$ standard, qui augmente toujours avec le nombre de paramètres et ce, même si les paramètres n'expliquent rien, le $R^{2}$ ajusté n'augmente pas avec le nombre de paramètres si ceux-ci ne sont d'aucune utilité pour expliquer le phénomène.

##### F-statistic

```{r}
summary(m)
```

<br>

Teste l'hypothèse nulle selon laquelle toute les pentes sont simultanément égales à zéro, i.e.:

<br>

$$ H_{0}: \beta_{1} = \beta_{2} = ... = \beta_{p} = 0 $$

<br>

Ce test  produit une valeur de $F$ qui représente un ratio de variances. Le rejet de cette hypothèse signifie qu'au moins une des pentes est différente de zéro. Intuitivement, cela peut être interprété comme signifiant que le modèle explique une portion significative de la variance. À noter que parfois, ce test peut être significatif, alors que les tests individuels sur chaque coefficient ne le sont pas ou vice versa. 

La function `anova` peut être utilisée pour effectuer ce test en l'appliquant sur un modèle ou sur plusieurs modèle pour comparer les modèles entre eux.

##### Variance résiduelle

```{r}
summary(m)
```

<br>

`Residual standard error` représente la variance résiduelle ou la variance non-expliquée par le modèle. Remarquez que cette valeur n'est pas très différente de la valeur de 5 qui a été utilisée pour simuler le modèle.

-->

#### Vérification des suppositions de bases

On peut utiliser la fonction `plot` qui lorsqu'elle est appliquée sur un objet de classe `lm` produira des graphiques permettant de vérifier certaines suppositions de bases ou conditions d'application.

```{r}

par(mfrow=c(2,2))
plot(m)

```

##### Fonctions accessoires

On peut également créer facilement certains de ces graphiques avec d'autres fonctions.

```{r,fig.width=12,fig.height=4}

par(mfrow=c(1,3))
plot(resid(m),fitted(m))
hist(resid(m))
qqnorm(resid(m))
qqline(resid(m))

```

### Facteurs

```{r}

# simulations d'un facteur
d$fac<-factor(sample(c("A","B","C"),nrow(d),replace=TRUE))
d$y<-d$y+2*as.integer(d$fac)-1

# modèle modifié avec le facteur
m<-lm(y~x1+x2+x3+fac,data=d)
summary(m)

```

<!-- Les facteurs sont comparés par rapport au niveau de référence! -->

## Comprendre son modèle avec des graphiques

La fonction `predict` sert à calculer la valeur prédite par le modèle pour chaque observation. Par exemple, ceci permet de mettre en relation les valeurs observées et les valeurs prédites. La fonction `predict` dans ce cas calcule la valeur prédite par le modèle pour chaque observation dans la base de donnée.

```{r}

plot(d$y,predict(m))

```


### Soumettre de nouvelle valeurs à `predict`

Plus souvent, on veut illustrer l'effet de nos variables et pour cela, il faut soumettre des valeurs à la fonction `predict`. Cette fonction est très utile lorsque l'on veut un maximum de contrôle sur les valeurs à prédire.

```{r}

x<-seq(min(d$x1),max(d$x1),length.out=50)
nd<-data.frame(x1=x,x2=mean(d$x2),x3=mean(d$x3),fac="B") # on fixe les autres variables à leur valeur moyenne
p<-predict(m,newdata=nd)
plot(d$x1,d$y) # observations
lines(x,p) # valeurs prédites

```


### Package `visreg`

Le package [visreg](https://pbreheny.github.io/visreg/) permet d'illustrer facilement les prédictions d'un modèle (ou les effets marginaux des différentes variables explicatives). Il utilise en arrière-plan la fonction `predict`.

```{r,fig.width=8,fig.height=8}
library(visreg)
par(mfrow=c(2,2))
visreg(m)

```


#### Package `ggeffects`

Le package [ggeffects](https://strengejacke.github.io/ggeffects/index.html) permet également d'illustrer les prédictions d'un modèle, mais ce package se base sur l'utilisation du package [ggplot2](https://ggplot2.tidyverse.org/) pour construire les graphiques. Le principe demeure le même: en arrière-plan la fonction `predict` est utilisée. On peut également combiner les différents graphiques générés en utilisant le package [patchwork](https://patchwork.data-imaginist.com/index.html) et sa fonction `wrap_plots`.

```{r,fig.width=8,fig.height=6,message=FALSE}
library(ggeffects)
library(patchwork)
g<-ggpredict(m)
p<-plot(g,add.data=TRUE)
wrap_plots(p)

```


### Interactions

Dans vos mots, qu'est-ce qu'une interaction?

<br>

C'est lorsque l'effet d'une variable dépend des valeurs d'une autre variable (ou de plusieurs).


#### Un exemple

```{r}

# modèle modifié avec le facteur
m<-lm(y~x1+x2+x3+x1*fac,data=d)
summary(m)

```


#### Illustration graphique

```{r}

visreg(m,"x1",by="fac",overlay=TRUE)

```

<br>

On a une interaction significative, mais est-elle importante d'un point de vue biologique? Peut-être pas... Toujours distinguer entre la **signification statistique** et la **signification biologique**.

Une valeur de p significative c'est bien beau, mais ce qui importe vraiment c'est **la taille de l'effet**.


##### Interactions avec `ggeffects`

```{r,message=FALSE}

g<-ggpredict(m,terms=c("x1","fac"),message=FALSE)
plot(g,add.data=TRUE,jitter=FALSE)

```


#### Interprétion

* Beaucoup plus facile avec un graphique qu'en regardant une table de coefficients.

* En présence d'une interaction (significative), les effets simples sont plus difficilement interprétables et ne peuvent être interprétés sans faire référence à l'interaction (à moins d'ajustements particuliers voir [Schielzeth 2010](https://doi.org/10.1111/j.2041-210X.2010.00012.x)).

* Si pour deux variables impliquées dans une interaction les effets simples ne sont pas significatifs, mais que leur interaction l'est, il faut considérer que ces deux variables ont un effet même si les coefficients associés aux effets simples ne sont pas significatifs.


## Types d'erreurs

<br>

* Erreur de type I: rejeter l'hypothèse nulle, alors qu'elle est vraie (faux positif).

<br>

* Erreur de type II: ne pas rejeter l'hypothèse nulle, alors qu'elle est fausse (faux négatif). 


### Simulations 

Simulons un modèle dans lequel les variables n'ont aucun effet. 

Que se passera-t-il si on répète plusieurs fois ce scénario?

À quoi ressembleront nos valeurs de p?

Créons d'abord une fonction `sim_model` pour générer un modèle linéaire fictif. 

```{r}

sim_model<-function(b0,b1,b2,b3){
  n<-200
  x1<-runif(n,0,100)
  x2<-runif(n,0,10)
  x3<-runif(n,0,1)
  mu<-b0+b1*x1+b2*x2+b3*x3
  y<-rnorm(n,mu,5)
  d<-data.frame(y,x1,x2,x3)
  m<-lm(y~x1+x2+x3,data=d)
  m
}
```


#### Simulons

Ensuite, simulons `nsims` modèles fictifs et emmagasinons ces modèles dans une liste nommée `list_models`.

```{r}
nsims<-500

list_models<-lapply(1:nsims,function(i){
  sim_model(b0=100,b1=0,b2=0,b3=0)  
})
```

<br>

Si on illustre les valeurs de p obtenues pour chacun des coefficients associés aux variables $x_{n}$ à l'aide d'un histogramme, à quoi ressemblera cet histogramme?


#### Valeurs de p obtenus


```{r}

p<-lapply(list_models,function(i){
  summary(i)$coef[2:4,4]
})

hist(unlist(p),breaks=seq(0,1,by=0.05),xlab="Valeurs de p")

```


##### Conclusions

* Dans une certaine proportion des cas ( ~ 5% ), on conclut à un effet, alors qu'il n'y en a pas pas! C'est le fameux seuil $\alpha$ de 0.05.

* Sous l'hypothèse nulle, les valeurs de p sont distribuées uniformément entre 0 et 1.

<!--

## Extensions

<br>

Quelles sont les suppositions de bases qui sont susceptibles de ne pas être rencontrées?

* Résidus non-distribués normalement

* Non-égalité des variances

* Non-indépendence des observations

* Relation non-linéaire 

* Etc.

<br>

À voir au prochain cours!

-->

<!-- # 2. Modèles linéaires généralisés et extensions -->

## Modèles linéaires standards

$$y \sim \mathcal{N}(\mu, \sigma^2)$$

$$\mu=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k$$


### Formulation matricielle

$$y \sim \mathcal{N}(\beta\mathbf{X}, \mathbf{I}\sigma^2)$$

<br>

$\mathbf{X}$ est une matrice avec les valeurs des variables explicatives.

$\mathbf{I}$ est une matrice identité qui a pour effet d'assigner une variance égale $\sigma^2$ à toutes les observations.


## Modèles linéaires généraux

$$y \sim \mathcal{N}(\beta\mathbf{X}, \mathbf{\Sigma})$$

$\mathbf{\Sigma}$ est une matrice de variance-covariance qui permet beaucoup plus de flexibilité en terme de variance et de dépendance entre les observations. En général, des paramètres seront associés à la matrice $\mathbf{\Sigma}$ pour permettre différentes situations. Dans le cas d'un modèle linéaire standard, on a $\mathbf{\Sigma} = \mathbf{I}\sigma^2$. 

<br>

Ex.: variance différente pour les niveaux d'un facteur, variance augmentant avec certaines valeurs de $x$, autocorrélation temporelle ou spatiale, etc.

<br>

Ces différentes situations peuvent être obtenues entre autres avec les packages [nlme](https://CRAN.R-project.org/package=nlme) (fonctions `gls` ou `lme`) ou [glmmTMB](https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html).   


## Modèles linéaires généralisés (GLM)

$$y \sim \mathcal{ExpFam}(\theta, ...)$$

$$\mathcal{g}(\mu)=\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k$$


## Structure d'un GLM

$$y \sim \mathcal{ExpFam}(\theta, ...)$$

<br>

$$ \eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k$$

<br>

$$ g(E(y)) = g(\mu) = \eta $$


### Composante systématique

$$ \eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k$$

Le prédicteur linéaire $\eta$ permet de relier les variables explicatives à la variable réponse.


### Composante aléatoire

$$ y \sim \mathcal{ExpFam}(\theta) $$

<br>

Les observations $y$ suivent une distribution particulière appartenant à la famille des distributions exponentielles avec des paramètres $\theta$. La distributions normale, de Poisson, Binomiale et Gamma font parties de la famille des distributions exponentielles.


### Fonction de lien

$$ g(E(y)) = g(\mu) = \eta $$

<br>

La fonction de lien (*link function*) est une transformation qui permet de relier la réponse attendue $E(y)$ (i.e. la moyenne) au prédicteur linéaire.

<br>

C'est ce qui permet de relier les variables explicatives à la variable réponse. Pour chaque distribution qui pourrait être utilisée dans le cadre d'un GLM, une fonction de lien dite canonique est utilisée par défaut, mais il existe plusieurs fonctions de liens possibles.



## GLM Poisson

$$ y \sim \mathcal{Pois}(\lambda) $$

<br>

$$ log(\lambda) = \eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k $$

<br>

On a donc des observations $y$ qui suivent une distribution de Poisson avec un paramètre $\lambda$. Une fonction de lien $log$ permet de relier linéairement les variables explicatives à la valeur attendue $E(y) = \lambda$ de la variable réponse.


### Distribution de Poisson

Cette distribution est souvent utilisée pour décrire le nombre de fois qu'un événement survient dans un intervalle de temps ou d'espace.

$$f(k,\lambda) = Pr(y = k) = \frac{\lambda^{k}e^{-\lambda}}{k!}$$

Cette fonction permet de calculer la probabilité d'observer un événement $k$ fois avec une distribution de Poisson ayant un paramètre $\lambda$. La moyenne de cette distribution est égale à $\lambda$.

Cette distribution a la propriété que $\lambda = E(y) = Var(y)$. Autrement dit, la moyenne et la variance sont égales et correspondent au paramètre $\lambda$ de la distribution.


#### D'où provient-elle?

Simulons une distribution de Poisson en distribuant aléatoirement des observations dans un carré 10 x 10. Ensuite, divisons ce carré en 100 cellules.

```{r, message=FALSE}

library(sf)

n<-200
x<-runif(n,0,10)
y<-runif(n,0,10)

sfc <- st_sfc(st_polygon(list(rbind(c(0,0), c(10,0), c(10,10), c(0,0)))))
grid <- st_make_grid(sfc, cellsize = 1)
plot(grid,axes=TRUE)
points(x,y)

```


#### Nb d'observations par cellule

Ensuite, calculons le nombre d'observations par cellule et illustrons la distribution du nombre d'observations pour chaque cellule. En moyenne, on devrait retrouver 2 observations par cellule (200 obs. / 100 cell. = 2).

```{r,fig.height=4,fig.width=4}

o<-st_intersects(grid,st_as_sf(data.frame(x,y),coords=c("x","y")))
co<-sapply(o,length)
hist(co,breaks=seq(-0.5,max(co)+0.5,by=1),freq=FALSE,main="",xlab="Nb d'observations")
lines(0:max(co),dpois(0:max(co),lambda=2),type="b",cex=2)
legend("topright",lty=c(NA,1),legend=c("Observée","Théorique"),bty="n",cex=1,pch=c(22,21),pt.bg=c("gray","white"))

```

Ce serait la même chose si on simulait des observations aléatoires dans le temps et si on comptait le nb d'observations par intervalles de temps.


### Simulons un modèle

Simulons un modèle à partir d'une distribution de Poisson.

```{r}

n<-100
x<-runif(n,0,10)
lambda<-exp(-2+0.4*x)
y<-rpois(n,lambda)
d<-data.frame(y,x)

m<-glm(y~x,family="poisson",data=d)
summary(m)

```

#### Coefficients

Les coefficients estimés sont très similaires à ce qui a été utilisé pour générer le modèle (-2 et 0.4).

```{r}

coef(m)

```


#### Prédictions

Illustrons l'effet de la variable explicative.

```{r}

visreg(m)

```


#### Sous quelle échelle?

```{r}

visreg(m,scale="response")

```

#### Échelle du prédicteur linéaire ou de la réponse

```{r,fig.height=5}

par(mfrow=c(1,2))
visreg(m)
visreg(m,scale="response")

```

Remarquez que dans le graphique de gauche, les valeurs en y sont négatives ou positives. En fait, la fonction log permet de transformer les comptes allant de 0 à l'$+\infty$ en valeurs allant de $-\infty$ à $+\infty$. Sous l'échelle log, les valeurs entre 0 et 1 sont négatives et les valeurs supérieures à 1 sont positives. 

$$ log(1) = 0 $$
$$ log(0.1) = -2.302586 $$
$$ log(5) =  1.609438 $$


#### `ggeffects`

```{r,message=FALSE}

g<-ggpredict(m,terms=c("x [n=50]"))
plot(g,add.data=TRUE,jitter=FALSE)

```


#### `predict`

```{r}

v<-seq(min(x),max(x),length.out=50)
plot(y~x,data=d)
p<-predict(m,data.frame(x=v))
lines(v,p)

```


#### Argument `type`

Par défaut, l'argument `type = "link"` et il faut spécifier `type = "response"` pour obtenir les prédictions sous l'échelle de la réponse.

```{r}

v<-seq(min(x),max(x),length.out=50)
plot(y~x,data=d)
p<-predict(m,data.frame(x=v),type="response")
lines(v,p)

```


### Suppositions de base

On pourrait être tenté d'utiliser la fonction `plot` pour vérifier les conditions d'applications.

```{r}

par(mfrow=c(2,2),mar=c(4,4,2,2))
plot(m)

```


### Quelles sont les suppositions de bases?

$$ y \sim \mathcal{Pois}(\lambda) $$

$$ log(\lambda) = \eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k $$

Certaines suppositions sont les mêmes que pour un modèle linéaire standard avec une distribution, mais d'autres diffèrent.


#### Vérification avec DHARMa

Une façon de vérifier les supposition de bases des modèles est d'utliser le modèle pour simuler des observations et ensuite comparer ces observations simulées aux observations réelles. Si les observations réelles ne ressemblent pas aux observations simulées à partir du modèle, c'est que le modèle ne représente pas bien ce qui se passe avec les données réelles et donc que qqch devrait être modifié dans le modèle.

Le package [DHARMa](https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html) se base sur ces simulations pour comparer les observations attendues par le modèle aux observations réelles. Plus spécifiquement, la méthode utilise les résidus quantiles pour comparer chaque observation aux observations attendues pour cette observations. Les résidus quantiles sont rapportés entre 0 et 1 et si le modèle est approprié pour les données, on devrait s'attendre à ce que les résidus soient distributés uniformément entre 0 et 1 pour toutes les observations.


```{r,message=FALSE}

library(DHARMa)

s<-simulateResiduals(m)
plot(s)

```

Si le modèle est appropré pour les données, on devrait voir ici une distribution uniforme des points et il ne devrait pas y avoir de patrons particuliers dans la distribution des points dans le graphique de droite. En particulier, les lignes de prédictions illustrées devraient être alignées sur les trois lignes représentant les quantiles 25, 50 et 75%. Dans le graphique quantile-quantile de gauche, les points devraient être alignés sur la droite. 


##### Autres vérifications

Les problèmes de sous- ou sur- dispersion et de surabondance de 0 (*zéro-inflation*) sont très fréquents avec les GLM. Ces deux fonctions de DHARMa permettent d'évaluer si les observations réelles diffèrent des observations simulées en terme de présence de 0 ou de disperison. En rouge, ce sont les observations réelles et en gris les observations simulées. Si les observations réelles sont bien en dehors des observations simulées, C'est qu'il y a un problème avec le modèle et il faut un ajustment.

```{r,message=FALSE,fig.height=5,results='hide',fig.keep='all'}

par(mfrow=c(1,2))
testDispersion(s)
testZeroInflation(s)

```


## GLM binomial

$$ y \sim \mathcal{Binom}(n,p) $$

$$ logit(p) = log\left(\frac{p}{1-p}\right) = \eta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k $$


### Régression logistique

$$ y \sim \mathcal{Binom}(1,p) = \mathcal{Bernoulli}(p)$$

La distribution de Bernoulli est un cas particulier de la distribution binomiale avec un tirage (e.g. un individu est vivant ou mort, un individu est mâle ou femelle, etc.)


### Distribution binomiale

$$f(k,n,p) = Pr(y = k) = \binom{n}{k}p^k(1-p)^{n-k}$$

où

$$\binom{n}{k}=\frac{n!}{k!(n-k)!}$$

<br>

Ce qui représente le nombre de façons de prendre $k$ éléments parmi $n$ éléments.

<br>

Cette fonction permet de calculer la probabilité d’observer $k$ succès si $n$ tirages sont faits et que la probabilité de succès est de $p$

<br>

Cette distribution a la propriété que $E(y) = np$ et $Var(y) = np(1-p)$ , autrement dit, la moyenne et la variance sont entièrement détemrinées par la propbabilité de succès et par le nombre de tirages. On a donc une autre distribution assez restrictive.


#### Exemple

Supposons qu'on tire pile (0) ou face (1) 10 fois et qu'on calcule le nombre de face et qu'on répète cette exercice 500 fois. Quelle sera la distribution du nombre de faces obtenus?

```{r, fig.height=4,fig.width=4}

n<-500
co<-sapply(1:n,function(i){
  s<-sample(0:1,10,prob=c(0.5,0.5),replace=TRUE)
  sum(s) # on additione les faces (1)
})

hist(co,breaks=seq(-0.5,max(co)+0.5,by=1),freq=FALSE,main="",xlab="Nb de faces")
lines(0:max(co),dbinom(0:max(co),prob=0.5,size=10),type="b",cex=2)
legend("topright",lty=c(NA,1),legend=c("Observée","Théorique"),bty="n",cex=1,pch=c(22,21),pt.bg=c("gray","white"))

```


### Simulons un modèle

```{r}

logit<-function(p){
  log(p/(1-p))
}

inv.logit<-function(x){
   exp(x)/(1+exp(x))  
}

n<-200
x<-runif(n,0,10)
z<--2+0.5*x
y<-rbinom(n,size=1,prob=inv.logit(z))

d<-data.frame(y,x)

m<-glm(y~x,family="binomial",data=d)
summary(m)

```


#### Visualisation

Avec `visreg`

```{r,fig.height=5}

par(mfrow=c(1,2))
visreg(m)
visreg(m,scale="response")

```

Remarquez que dans le graphique de gauche, les valeurs en y sont négatives ou positives. En fait, la fonction logit permet de transformer les probabilités allant de 0 à 1 en valeurs allant de $-\infty$ à $+\infty$. Sous l'échelle logit, une valeur de 0 corrrespond à une probabilité de 0.5.

$$ log\left(\frac{p}{1-p}\right) = log\left(\frac{0.5}{1-0.5}\right) = log(1) = 0 $$


#### Visualisation

Avec `ggeffects`

```{r,message=FALSE}

g<-ggpredict(m,terms=c("x [n=50]"))
plot(g,add.data=TRUE)

```


### Suppositions de base

```{r,fig.height=8,results='hide',fig.keep='all'}

par(mfrow=c(2,2))
s<-simulateResiduals(m)
testUniformity(s)
testQuantiles(s)
testDispersion(s)
testZeroInflation(s)

```


## Problèmes

Les problèmes les plus fréquents sont la **surdispersion** et la **surabondance de zéros** (*zero-inflation*)


### Surdispersion

C'est lorsque les données sont plus variables que ce qui est attendu par le modèle. Les distribution de Poisson et binomiale sont très restrictives par rapport à leur variance et les problèmes de surdispersion sont très fréquents.

<br>

En général, une surdisperion ignorée fera en sorte que l'incertitude sera sous-estimée (trop de confiance dans nos résultats).

<br>

Plusieurs causes possibles (variables explicatives manquantes, phénomènes d'aggrégation, hétérogénéité, etc.)

<br>

Solutions: 

Poisson -> Négative binomiale (`glm.nb`, `glmmTMB`, `brms`, etc.)

Binomiale -> Béta binomiale (`glmmTMB`, `brms`, etc.)

*Obervation-Level Random Effects* (OLRE) (voir [Harrison 2015](https://doi.org/10.7717/peerj.1114) et [Harrison 2014](https://doi.org/10.7717/peerj.616)).


#### Exemple

Voici un exemple de GLM de Poisson avec de la surdispersion. Pour créer cette surdispersion, j'utilise une distribution négative binomiale pour générer les observations au lieu d'une distribution de Poisson. 

```{r,fig.width=6,fig.height=5,message=FALSE}

n<-100
x<-runif(n,0,10)
lambda<-exp(-2+0.4*x)
y<-rnbinom(n,mu=lambda,size=0.5) # 
d<-data.frame(y,x)

m<-glm(y~x,family="poisson",data=d)
g<-ggpredict(m,terms="x")
plot(g,add.data=TRUE)

```


#### Vérification avec DHARMa

Les résidus ne sont clairement pas distributés uniformément sur le second graphique et la surdispersion est évidente sur le 3e graphique.

```{r,fig.width=12,fig.height=3,results='hide',fig.keep='all'}

s<-simulateResiduals(m)
par(mfrow=c(1,4))
testUniformity(s)
testQuantiles(s)
testDispersion(s)
testZeroInflation(s)

```


#### Correction avec glmmTMB

```{r,fig.width=12,fig.height=3,results='hide',fig.keep='all'}

library(glmmTMB)

m<-glmmTMB(y~x,family="nbinom1",data=d)

s<-simulateResiduals(m)

par(mfrow=c(1,4))
testUniformity(s)
testQuantiles(s)
testDispersion(s)
testZeroInflation(s)

```


### Sousdisperion

C'est lorsque les données sont moins variables que ce qui est attendu par le modèle. Ceci est beaucoup plus rare et probablement moins dommageable (e.g. taille de couvée/portée chez les oiseaux/mammifères voir [Brooks et al. 2019](https://doi.org/10.1002/ecy.2706)). 

<br>

En général, une sous-dispersion ignorée fera en sorte que l'incertitude sera surestimée (trop conservateurs dans nos résultats) ce qui est un moindre mal.

<br>

Solutions: 

Poisson -> `glmmTMB, family = "compois"`
 
 
### Zéro-inflation

Surabondance de 0 dans les observations par rapport à ce qui est attendu par le modèle. Il est possible aussi d'avoir une sous-abondance de 0 et/ou encore une surabondance de 1, mais ce dernier cas est un peu plus rare.

<br>

Solutions: 

modèle *zero-inflated* (`glmmTMB`,`pscl`,`brms`, etc.)

<br>

À noter qu'une surdipersion peut être due à une surabondance de 0 et vice-versa. Ce sont deux problèmes qui peuvent interagir.


### Extensions

<br>


**Gamma** &nbsp;&nbsp; $]0,\infty$

Variable réponse continue, strictement positive et avec une asymétrie à droite.

Souvent une alternative à une transformation $log$.

Ex.: concentration d'un composé, biomasse, etc.

<br>


**Tweedie** &nbsp;&nbsp; $[0,\infty$

Variable réponse continue positive et incluant des 0.

Ex.: biomasse capturée, précipitations, etc.

Voir [Lecomte et al. 2013](https://doi.org/10.1111/2041-210X.12122)

<br>


**Beta** &nbsp;&nbsp; $]0,1[$ &nbsp; ou &nbsp; $[0,1]$

Variable continue entre 0 et 1 exclusivement.

Pour proportion ou % ne provenant pas d'un ratio entre entiers.

Si 0 et 1 possibles, certaines transformations peuvent être utilisées.

Ex.: proportion d'une surface affectée, proportion de temps passé en alimentation, etc.

Voir [Douma & Weedon 2019](https://doi.org/10.1111/2041-210X.13234)

<!-- 

## Modèles généraux additifs (GAM)

Les GAMs (*Generalized Additive Models*) sont utiles lorsque la relation entre les variables explicatives et la variable réponse est non-linéaire ou complexe et qu'elle ne peut pas être décrite à l'aide d'un modèle théorique.

<br>

Des exemples? 

<br>

Pour décrire le lien entre une variable $x$ et la variable réponse, des fonctions de lissage sont utilisées (*smooth functions*).



### Exemple

Encore une fois, simulons des données fictives en créant une relation complexe entre $x$ et $y$.

```{r}

set.seed(123) 
n<-200 
x<-sort(runif(n,0,10)) 

f<-function(i){ # fonction complexe reliant x et y
  cos(i)+exp(0.2*i)
}

y<-f(x)+rnorm(length(x),0,0.5) # on ajoute un peu de variation autour de cette fonction
d<-data.frame(x,y) 

plot(y~x,data=d,xlab="x",ylab="y",col="grey60")
v<-seq(min(d$x),max(d$x),by=0.1)
lines(v,f(v),col="grey60",lwd=2) 

```


#### Un modèle linéaire?


```{r}

plot(y~x,data=d,xlab="x",ylab="y",col="grey60")
lines(v,f(v),col="grey60",lwd=2) ## generating function (TRUE function)
m<-lm(y~x,data=d)
p<-predict(m,data.frame(x=v))
lines(v,p,lwd=1)

```

#### Vérifions les suppositions

```{r}
par(mfrow=c(2,2))
plot(m)

```

#### Un modèle polynomial?

```{r}

plot(y~x,data=d,xlab="x",ylab="y",col="grey60")
lines(v,f(v),col="grey60",lwd=2) ## generating function (TRUE function)

m2<-lm(y~poly(x,2),data=d)
p<-predict(m2,data.frame(x=v))
lines(v,p,col=2)

m3<-lm(y~poly(x,3),data=d)
p<-predict(m3,data.frame(x=v))
lines(v,p,col=3)

m4<-lm(y~poly(x,4),data=d)
p<-predict(m4,data.frame(x=v))
lines(v,p,col=4)

legend("topleft",lwd=1,cex=2,col=2:4,legend=c(expression(x^2),expression(x^3),expression(x^4)),bty="n")

```

#### Un GAM!

Maintenant, utilisons plutôt un GAM avec la fonction `gam` du package [mgcv](https://CRAN.R-project.org/package=mgcv).

```{r,message=FALSE}

library(mgcv)

plot(y~x,data=d,xlab="x",ylab="y",col="grey60")
lines(v,f(v),lwd=2)

m<-gam(y~s(x),data=d) # le s veut dire smooth de x

p<-predict(m,data.frame(x=v))
lines(v,p,col="red")

```


### Formulation

On a un prédicteur linéaire fait de fonctions non-linéaires (*smooths*) des variables explicatives.

$$y \sim \mathcal{ExpFam}(\mu,...)$$

$$\mathcal{g}(\mu)=\beta_0 + \mathcal{f_1} (X_1) + \mathcal{f_2} (X_2) + ... + \mathcal{f_n}(X_n)$$

Il y a plusieurs façons de construire ces fonctions. Chaque fonction $\mathcal{f}$ a une base d'une dimension donnée $\mathcal{q}$ pour une variable $X$.

$$\mathcal{f} (X)=\sum_{i=1}^{\mathcal{q}} \mathcal{b_i}(X)\beta_i$$

où les $\beta_i$ sont des fonctions arbitraires. Ainsi, un GAM est une combinaison linéaire de termes de lissage. Un exemple de base pourrait être par exemple un polynome de degré 3.

$$\mathcal{f} (X)=\beta_1 + \beta_2 X + \beta_3 X^2 + \beta_4 X^3$$


#### Fonctions de base?

Ici, on extrait les fonctions de bases du modèle (*basis functions*). 

```{r}
mm <- predict(m, type = "lpmatrix")

matplot(d$x,mm[,-1],type="l",ylab="Basis functions")

nn <- ncol(mm[,-1])
legend("top",horiz=TRUE,colnames(mm[,-1]),cex=0.8,col=seq_len(nn),lty=seq_len(nn),inset=c(0,-0.1),xpd=TRUE,bty="n")
```


#### Combinons ces fonctions

On les combine ensuite avec les paramètres du modèle pour estimer la relation complexe entre x et y.

```{r}
plot(y~x,data=d)
lines(d$x,mm%*%m$coefficients)
```


<!-- One might ask, why use piecewise polynomial instead of a high degree polynomial fitted to the whole data? High degree polynomials can lead to unwanted oscillations, notably at the edges of predictions. For example, see [Runge's phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon).   -->

<!--

#### Pourquoi ?

Pourquoi utiliser les GAMs vs. la régression polynomiale?

- Régression polynomiale plus instable et subjective.

<br>

Détermination automatique du niveau optimal de lissage (avec le package mgcv).


#### `mgcv`

Le package `mgcv` est très bien documenté et son fichier d'aide contient beaucoup d'information.

`?mgcv` information générale sur le package

`?gam` information générale sur la fonction `gam`

`?gam.models`  comment spécifier un modèle avec `gam`

`?gam.selection`  comment faire de la sélection de modèle
 
`?mgcv.FAQ`  questions fréquemment posées

-->

<!--

# 3. Sélection de modèles


## But d'une analyse

- Tester des hypothèses

- Faire des prédictions

- Faire de l'exploration

<br>

Les méthodes d'analyses statistiques utilisées et la façon de présenter les résultats devraient être influencées par le but de l'analyse.


### Tester des hypothèses

Ici, on a généralement des hypothèses a *priori* que l'on veut tester avec des données.


### Faire des prédictions

Ici, le but est surtout de prédire et de trouver les meilleurs modèles / variables maximisant la capacité prédictive. La capacité prédictive est la priorité.


### Faire de l'exploration

Ici, le but est surtout de découvrir des relations dans un contexte ou les connaissances théoriques ne sont pas assez développées pour générer des hypothèses claires. La découverte de relations servira à proposer de nouvelles hypothèses ou de nouveaux mécanismes à explorer.


## Compromis biais vs. variance

En omettant des variables d'un modèle, on risque de biaiser l'estimation des effets des variables incluses.

<br>

En incluant trop de variables dans un modèle, on augmente l'incertitude dans nos estimations.

<br>

Il y a donc toujours un compromis entre estimer nos paramètres avec précision et sans biais.


## Surparamétrisation / sousparamétrisation

On parle souvent d'*overfitting* ou d'*underfitting*.

Un modèle surparamétrisé est un modèle qui "colle" trop bien aux données. Soit que le modèle a trop de paramètres, soit que le modèle est trop flexible. Dans tous les cas le modèle s'adapte trop bien au jeu de données en question. 

<br>

Le danger est qu'en apprenant un jeu de données trop bien, le modèle risque de capturer les caractéristiques particulières d'un jeu de données ce qui fera en sorte que le modèle sera difficilement généralisable à d'autres situations (e.g. faire de bonnes prédictions avec d'autres données ou expliquer ce qui est observé dans un autre système).


## Parsimonie

Le principe de parsimonie guide une bonne partie de la science. Ce principe veut que les explications simples sont généralement préférables aux explications complexes (ou compliquées ?).

<br>

En statistiques, un modèle parsimonieux est un modèle qui explique beaucoup avec peu de paramètres (mais, voir [Coelho et al. 2018](https://doi.org/10.1111/ecog.04228)).


## Sélection de modèles

Simulons une régression linéaire simple

```{r}
set.seed(12345)

n<-100
x1<-runif(n,0,10)
y<-rnorm(n, mean=10+2*x1, sd=5)
d<-data.frame(y,x1)
m <- lm(y~x1,data=d)
summary(m)
```

### Illustration 

```{r}
plot(x1,y)
abline(m)
```


### R2

Calculons un R2 à partir de cette régression. Il y a plusieurs façons de le calculer. Ici, on le calcule à partir d'une corrélation entre les valeurs observées et les valeurs prédites.

```{r}
plot(y,fitted(m))
cor(y,fitted(m))^2
```


#### À partir de la variance expliquée

```{r}
a<-anova(m)
a
names(a)
a[1,2]/(a[1,2]+a[2,2])

```


#### Ajout de variables

Que se passe-t-il si on ajoute des variables n'ayant aucun effet sur *y* dans le modèle?

```{r}

x<-lapply(2:10,function(i){
  runif(n,0,10)  
})
x<-do.call("data.frame",x)
names(x)<-paste0("x",2:10)
d<-cbind(d,x)

r2<-lapply(2:10,function(i){
  f<-as.formula(paste("y~",paste(names(d)[2:i],collapse="+")))
  m2<-lm(f,d)
  summary(m2)["r.squared"]
  as.data.frame(summary(m2)[c("r.squared","adj.r.squared")])
})
r2<-do.call("rbind",r2)

matplot(r2,type="b",xlab="Nb de paramètres",ylab="R2",col=1:2,lwd=2,pch=1,cex=2,lty=1)
legend("topleft",inset=c(0.05,0.025),col=1:2,lwd=2,legend=c("R2","R2 ajusté"),bty="n",cex=1.5,pch=21,pt.bg="white",pt.cex=2)

```

<br>

On voit que le R2 augmente à chaque ajout de variable. Sélectionner un modèle sur la base du R2 favorise le surparamétrage des modèles et la découverte d'effets fortuits.


#### Presque autant de variables que de paramètres?

Que se passe-t-il si on ajoute presque autant de variables explicatives que d'observations? Remarquez ici que je ne conserve que 10 observations dans le jeu de données.

```{r}
m<-lm(y~x1+x2+x3+x4+x5+x6+x7+x8,data=d[1:10,])
summary(m)
```


#### Autant de variables que de paramètres?

Si on en ajoute une de plus?

```{r}
m<-lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9,data=d[1:10,])
summary(m)

```

<br>

On a autant de paramètres que d'observations, alors on explique parfaitement les observations à partir du modèle!


#### Prédictions parfaites

Ici, notre modèle épouse parfaitement les données. Les valeurs prédites sont mêmes égales aux observations.

```{r}

plot(d$y[1:10],predict(m))

```

<br>

Bref, il est dangeureux de se fier au R2 pour sélectionner un meilleur modèle.


### Tests de ratio de vraisemblance

Intuitivement, ce test sert à évaluer si un modèle explique significativmenet plus de variance qu'un autre ou encore si un modèle a significativement plus de chances d,avoir générer les données qu'un autres. On parle souvent de (*Likelihood Ratio Test*). On peut utiliser la fonction anova appliquée sur plusieurs modèles pour effectuer ce test.

```{r}

m2<-lm(y~x1+x2,data=d)
m1<-lm(y~x1,data=d)
m0<-lm(y~1,data=d)
anova(m0,m1,m2)

```

<br>

Ce test nous dit que le modèle `m1` est nettement meilleur que le modèle `m0`,mais que le modèle `m2` n'est pas significativement meilleur que le modèle `m1`.

Ces tests ne peuvent être appliqués que sur des modèles nichés, *i.e.* des modèles qui contiennent des sous-ensembles de variables par rapport à un autre.


### Méthodes *stepwise*

Ces méthodes consistent à enlever ou ajouter successivement les variables d'un modèle en se basant sur le niveau de significativité des variables. On peut soit partir d'un modèle nul et ajouter des variables en fonction de leur significativité (*forward stepwise*), soit partir d'un modèle complet et enlever les variables ayant les effets les moins significatifs (*backward stepwise*). Différentes variations sur ces stratégies sont possibles.

<br>

Il est largement démontré que dans bien des cas, ces méthodes contribuent à surestimer l'effet des variables significatives, à augmenter le taux d'erreurs de type I et mènent souvent à des résultats instables (voir notamment [Whittingham et al. 2006](https://doi.org/10.1111/j.1365-2656.2006.01141.x) et [Mundy & Nunn 2009](https://doi.org/10.1086/593303)).


### AIC (critères d'information)

Le critère d'information d'Akaike (*Akaike Information Criterion*) se calcule de la façon suivante. 

<br>

$$AIC = -2\textrm{log}(\hat{L}) + 2p$$

Où $\hat{L}$ est le maximum de vraisemblance et $p$ le nombre de paramètres dans le modèle.

<br>

Il faut voir $2p$ comme étant un terme qui pénalise la valeur d'$AIC$ pour chaque paramètre ajouté au modèle.

<br>

Intuitivement, l'AIC sert à évaluer le pouvoir prédictif d'un modèle. Asymptotiquement, cette méthode est équivalente à la *Leave-one-out Cross-Validation*.


#### AICc

$$AICc = AIC + \frac{2p(p+1)}{n-p-1}$$

<br>

En pratique, on utilise souvent cette version qui est corrigée pour les petites tailles d'échantillon.


#### Exemple

On peut utiliser entre autres le package [AICcmodavg](https://CRAN.R-project.org/package=AICcmodavg) pour comparer des modèles par AIC(c).

```{r}
m1<-lm(y~1,data=d)
m2<-lm(y~x1,data=d)
m3<-lm(y~x1+x2,data=d)
m4<-lm(y~x1+x2+x3,data=d)
m5<-lm(y~x1+x2+x3+x4,data=d)
m6<-lm(y~x1+x2+x3+x4+x5,data=d)

library(AICcmodavg)

aictab(list(m1=m1,m2=m2,m3=m3,m4=m4,m5=m5,m6=m6))

```

En pratique, ce sont les différences entre les valeurs d'AICc (`Delta_AICc`, $\Delta\textrm{AICc}$) qui comptent et non les valeurs. On peut également utiliser le poids d'Akaike (`AICcWt`, $\omega_{i}$) qui représente le support relatif pour chacun des modèles. Le nombre de paramètres de chaque modèle est donné par `K`. 


#### Comment utiliser l'AIC

En général, l'AIC sert à évaluer quels modèles sont le mieux supportés par les données parmi l'ensemble des modèles mis en compétition. Parfois, un modèle se démarque clairement du lot, parfois, plusieurs modèles ont un support équivalent.

<br>

Avant de vous lancer dans la sélection de modèles par AIC, une bonne idée est de consulter la [vignette](https://cran.r-project.org/web/packages/AICcmodavg/vignettes/AICcmodavg.pdf) du package AICcmodvg qui contient les bonnes pratiques à utiliser.

<br>

##### Approche recommandée

L'approche recommandée est d'étudier un nombre restreint de modèles qui mettent en contraste différentes hypothèses.

<br>

Ex: on regroupe les variable par thèmes et on contraste différents modèles basés sur ces thèmes.


##### Approche plus exploratoire

On étudie toutes les combinaisons possibles de variables afin d'identifier le ou les meilleurs modèles.

<br>

- Plus grand risque de tomber sur des modèles jugés importants par hasard.

- En pratique, le nombre de combinaisons possibles peut rapidement devenir impossible à étudier.


##### Inférence multi-modèle

Lorsque plusieurs modèles ont un support équivalent, il peut être utile de faire de l'inférence multi-modèle (*model-averaging*). Cette méthode consiste à calculer une moyenne pour tous les paramètres ou les prédictions des modèles à travers tous les modèles étudiés.

<br>

Au premier abord, cette méthode peut sembler simple et attrayante, mais elle doit être utilisée avec beaucoup de discernement (voir entre autres [Cade 2015](https://doi.org/10.1890/14-1639.1) et [Dormann et al. 2018](https://doi.org/10.1002/ecm.1309)). La question de comment procéder à l'inférence multi-modèle est toujours une question ouverte.


#### BIC

Malgré son nom, le BIC (*Bayesian Information Criterion*) n'est pas du tout bayésien. 

<br>

$$BIC = -2\textrm{log}(\hat{L}) + p\textrm{log}(n)$$

<br>

C'est un autre critère qui se base sur des suppositions différentes par rapport à l'AIC. En pratique, le BIC va souvent mener à des modèles plus simples (plus parsimonieux?) que l'AIC.

L'AIC cherche à trouver le modèle avec le meilleur pouvoir prédictif parmi ceux proposés, alors que le BIC cherche à identifier le véritable modèle qui génère les données. L'interprétation de ces différentes métriques n'est pas toujours sans ambiguité.

Il existe pleins d'autres critères d'information pouvant être utiles dépendemment des applications (QAIC, DIC, WAIC, etc.)


### Validation croisée

Le principe derrière la validation croisée est d'entraîner un modèle sur une partie des données et de tenter de prédire les valeurs des données qui n'ont pas servi à entraîner le modèle.

<br>

Ceci permet de quantifier l'erreur de prédiction avec des mesures comme la somme des carrés moyenne (*root mean square error*, RMSE). Ici, on peut par exemple chercher le modèle minimisant cette erreur.

<br>

$$RMSE = \sqrt{\sum_{i=1}^{n} \frac{(\hat{y_{i}}-y_{i})^2}{n}}$$

<br>

Ici, on cherche à minimiser l'écart entre les prédictions et les observations


#### Plusieurs variantes

<br>

*Leave-one-out Cross-Validation*

Ici, on entraîne le modèle sur toutes les observations sauf une et on tente de prédire cette observation mise de côté. On répète ce processus pour toutes les observations et on calcule l'erreur globale ou moyenne. 

<br>

*K-fold Cross-Validation*

Ici, on divise le jeu de données en k parties qu'on laisse de coté et on entraîne le modèle sur le reste des données. On répète le processus pour chacune des k parties.


### Validation avec des données externes

<br>

On parle parfois de *Out-of-sample prediction*. 

Ici, on valide le pouvoir prédictif de notre modèle à l'aide d'un jeu de données externes à ce qui a été utilisé pour estimer les modèles. C'est la méthode la plus robuste, mais en pratique, ce n'est pas toujours possible.

Exemples?


### Régularisation

La régularisation est un terme général qui désigne l'ensemble des méthodes qui cherche à éviter la surparamétrisation. Ici, l'idée est de faire en sorte que les modèles identifient les structures "régulières" dans les données et non les structures "irrégulières".


On parle aussi souvent de pénalisation. L'idée est de pénaliser les modèles (i.e. les paramètres) pour éviter leur trop grande complexité.


#### Rappel régression

On cherche à minimser la somme des carrés résiduelles (RSS, *residual sum of squares*).

<br>

$$RSS = \sum_{i=1}^{n} \left(y_{i}-\sum_{j=1}^{p}\beta_{j}x_{ij}\right)^2$$

<br>

En d'autres mots, on cherche à minimser l'écart entre les prédictions et les observations.


#### *Ridge regression*

Avec la *ridge regression*, on ajoute une pénalité constituée de la somme des paramètres au carré et d'un paramètre $\lambda$ servant à faire l'ajustement. Le paramètre $\lambda$ est ajusté par validation croisée afin d'identifier la valeur optimale permettant le meilleur pouvoir prédictif.

<br>

$$Ridge = RSS + \lambda\sum_{j=1}^{p}\beta_{j}^2$$

<br>

Toutes les variables seront retenues dans ce modèle, mais les coefficients seront pénalisés vers 0 ce qui aura pour effet de réduire considérablement la contribution de certaines variables.


#### LASSO

Le LASSO (*Least Absolute Shrinkage and Selection Operator*) ressemble beaucoup à la *ridge regression*, mais cette fois la pénalité est construite avec la somme en valeur absolue des paramètres. 

<br>

$$LASSO = RSS + \lambda\sum_{j=1}^{p}\lvert\beta_{j}\rvert$$

<br>

Avec le LASSO, seules les variables les plus importantes seront retenues et les autres seront réduites à zéro. Ceci a l'avantage de procéder à une sélection automatique des variables.


#### Régression régularisée

La méthode *Elasticnet* est une sorte de compromis entre le LASSO et la *ridge regression*. Toutes ces méthodes sont accessibles via le package [glmnet](https://CRAN.R-project.org/package=glmnet).

<br>

Beaucoup utilisé en apprentissage machine (*machine learning*) étant donné que le but est très souvent la prédiction.

<br>

Le principal désavantage de ces méthodes est qu'il n'est pas clair pour l'instant comment devrait être estimée l'incertitude autour des paramètres ce qui en fait des outils moins utiles pour l'inférence.


## Conclusion

<br>

*No Free Lunch Theorem !*

<br>

Il n'y a pas une méthode qui performe mieux que les autres dans toutes les situations.

<br>

Malgré ce qu'on pourrait penser, la sélection de modèles (ou l'identification de variables importantes) est toujours une question ouverte en constante évolution. Il faut se méfier des traditions et de leur inertie...

<br>

Voir notamment cette conférence très intéressante par Mark Brewer concernant le culte de l'AIC: [Model selection and the cult of AIC](https://youtu.be/lEDpZmq5rBw)

<!--

### La fonction `gam`

Formulation similaire à un modèle linéaire stantard. On peut également inclure des variables sans fonctions de lissages.

```{r}
g<-gam(y~x,data=d)
summary(g)
```

<hr>

What does the summary output looks like with a smooth term?
```{r}
g<-gam(y~s(x),data=d)
summary(g)
```

1. The first part deals with parametric coefficients (factors, non-smoothed variables)

2. The second part deals with smoothed terms and report the **edf** which stands for **e**stimated **d**egrees of **f**reedom. 

Without going into details, it roughly represents the level of smooth complexity required for modeling the response.

3. The third part reports $R^2$-type measures and the generalized cross validation value.

<hr>

Several methods (functions) are available to deal with gam objects: 

- `print` 
- `summary`
- `anova` 
- `plot` 
- `predict`
- `residuals` 

<hr>

### Smooths

<br>

`s()`

`gam(y~s(x),data=d)`

Used for univariate smooths, isotropic smooths of several numeric variables (with variables on the same scale, e.g., XY coordinates), interactions, simple random effects.

<br>

`te()`

`gam(y~te(x,z),data=d)`

Tensor product smooths of several variables (interactions) e.g. 

<br>

`ti()`

`gam(y~ti(x)+ti(z)+ti(x,z),data=d)`

Tensor product smooths with marginal smooths excluded (and lower order interaction terms)

<br><br>

The list of available smooth terms is given in `?smooth.terms`


### Set up a model

All families available in GLMs are also available in GAMs and others are also available.

`?family.mgcv`


-->

<!--

-->

# 2. Modèles mixtes/hiérarchiques

<br><br>
$$ y = \alpha + \beta x + \epsilon $$

$$ \epsilon \sim \text{Normale}(0,\sigma^{2}) $$
<br>
<p style='text-align: center'>ou</p>
<br>
$$ \mu = \alpha + \beta x $$

$$ y \sim \text{Normale}(\mu,\sigma^{2}) $$


## Modèle linéaire hiérarchique simple

<br><br>
$$ y_{ij} = \alpha + \beta x_{ij} + \tau_{i} + \epsilon_{ij}$$

$$ \tau_{i} \sim \text{Normale}(0,\,\sigma_{\tau}^{2}) $$

$$ \epsilon_{ij} \sim \text{Normale}(0,\sigma_{\epsilon}^{2})\ $$

<br><br>
<p style='text-align: center'>où il y a $i$ groupes et $j$ observations par groupe</p>

## Autre formulation

<br><br>
$$ \mu_{ij} = \alpha_{i} + \beta x_{ij}$$

$$ \alpha_{i} \sim \text{Normale}(\alpha,\,\sigma_{\alpha}^{2})\ $$

$$ y_{ij} \sim \text{Normale}(\mu_{ij},\,\sigma_{\epsilon}^{2})\ $$
<br><br>
<p style='text-align: center'>Un modèle est dit hiérarchique lorsque certains de ses paramètres sont définis par une probabilité de distribution.</p>

```{r, eval = FALSE, include = FALSE, message = FALSE, warning = FALSE}
library(lme4)
library(ggeffects)


set.seed(1234)
ng<-50
nobs<-50
n<-ng*nobs
id<-gl(ng,nobs)

# valeurs fictives des variables explicatives selon une distribution uniforme
x1<-runif(n,0,100)
x2<-runif(n,0,10)
x3<-runif(n,0,1)

# valeurs des paramètres beta
b0<-100
b1<-0.2
b2<-1
b3<--10

reb0<-rep(rnorm(ng,0,15),each=nobs)
reb1<-rep(rnorm(ng,0,0.25),each=nobs)
reb2<-rep(rnorm(ng,0,0.01),each=nobs)
reb3<-rep(rnorm(ng,0,1),each=nobs)

# prédicteur linéaire
mu<-(b0+reb0)+(b1+reb1)*x1+(b2+reb2)*x2+(b3+reb3)*x3

# observations à partir d'une distribution normale avec un écart-type de 5
y<-rnorm(n,mu,5)

# données
d<-data.frame(y,x1,x2,x3)

m<-lmer(y~x1+x2+x3+(1|id),data=d)

g<-ggpredict(m,terms=c("x1","id [sample=20]"),type="random")
plot(g, add.data = TRUE, show_ci = FALSE, colors = rep("black",20), show_legend = FALSE)

m<-lmer(y~x1+x2+x3+(x1|id),data=d)

g<-ggpredict(m,terms=c("x1","id [sample=20]"),type="random")
plot(g, add.data = TRUE, show_ci = FALSE, colors = rep("black",20), show_legend = FALSE)

```

## Simulons un exemple

```{r, message = FALSE, warning = FALSE}

set.seed(12345)
ng <- 20
nobs <- 10
n <- ng * nobs
id <- gl(ng, nobs)

# valeurs fictives des variables explicatives selon une distribution uniforme
x <- runif(n, 0, 100)

# valeurs des paramètres
a <- 100
b <- 1

# on génère l'effet aléatoire pour chaque groupe
rea <- rep(rnorm(ng, 0, 20), each = nobs)

# prédicteur linéaire 
mu <- (a + rea) + b * x

# observations à partir d'une distribution normale avec un écart-type de 5
y <- rnorm(n, mu, 5)

# données
d <- data.frame(y, x)

```

## Illustrons le modèle

```{r, message = FALSE, warning = FALSE}

library(lme4)
library(ggeffects)

m <- lmer(y ~ x + (1|id), data = d)

g <- ggpredict(m, terms = c("x", "id [sample=20]"), type = "random")
plot(g, add.data = TRUE, show_ci = FALSE, colors = rep("black", 20), show_legend = FALSE)

```


## Illustrons les pentes par individus

```{r, message = FALSE, warning = FALSE}

g <- ggpredict(m, terms = c("x", "id [sample=20]"), type = "random")
plot(g, add.data = TRUE, show_ci = FALSE, colors = sample(colors(), 20))

```


## Retrouvons les paramètres

La variance de l'effet aléatoire était de 20² et la variance résiduelle était de 5².
```{r, message = FALSE, warning = FALSE}

summary(m)

```

## Récupérons ces effets

```{r, message = FALSE, warning = FALSE}
fixef(m)
ranef(m)
apply(ranef(m)$id, 2, sd)
```

## Coefficients

```{r, message = FALSE, warning = FALSE}
coef(m)
```


## Complexifions le modèle

```{r, message = FALSE, warning = FALSE}
b

reb <- rep(rnorm(ng, 0, 0.5), each = nobs)

mu <- (a + rea) + (b + reb) * x

y <- rnorm(n, mu, 5)

d <- data.frame(y, x)

```


## Illustrons les pentes

```{r, message = FALSE, warning = FALSE}

m <- lmer(y ~ x + (x|id), data = d)

g <- ggpredict(m, terms = c("x", "id [sample=20]"), type = "random")
plot(g, add.data = TRUE, show_ci = FALSE, colors = rep("black",20), show_legend = FALSE)

```


## Retrouvons les paramètres

La variance de l'effet aléatoire sur la pente était de (0.5)².
```{r, message = FALSE, warning = FALSE}

summary(m)

```


## Récupérons ces effets
```{r, message = FALSE, warning = FALSE}
fixef(m)
ranef(m)
apply(ranef(m)$id, 2, sd)
```


```{r, message = FALSE, warning = FALSE}
coef(m)
```


## Formulations

```{r, message = FALSE, warning = FALSE, eval = FALSE}

m <- lmer(y ~ x + (1|id), data = d)

m <- lmer(y ~ x + (x|id), data = d) # équivalent à (1 + x|id)

m <- lmer(y ~ x + (0 + x|id), data = d)

m <- lmer(y ~ x + (1|id) + (0 + x|id), data = d)

```

## Corrélations _intercepts_/pentes

```{r, message = FALSE, warning = FALSE, eval = FALSE}

m <- lmer(y ~ x + (1|id) + (0 + x|id), data = d)

```
<br>
Le dernier est équivalent à
<br>
```{r, message = FALSE, warning = FALSE, eval = FALSE}

m <- lmer(y ~ x + (x||id), data = d)

```
<br>
et signifie qu'on assume que l'_intercept_ et la pente ne sont pas corrélées ou qu'on ne veut pas estimer cette corrélation. En pratique, il n'y a pas énormément de raison de faire cela et par défaut le modèle va estimer cette corrélation.

## Effets aléatoires croisés

Supposons que nous avons des individus observés plusieurs fois par année et ce sur plusieurs années.

```{r, message = FALSE, warning = FALSE, eval = FALSE}

m <- lmer(y ~ x + (1|id) + (1|year), data = d)

```

## Effets aléatoires nichés

Supposons maintenant que les individus sont annuels et qu'ils ne sont observés que pour des années particulières.
<br>
```{r, message = FALSE, warning = FALSE, eval = FALSE}

m <- lmer(y ~ x + (1|year/id), data = d)

```
<br>
Équivalent à:
<br>
```{r, message = FALSE, warning = FALSE, eval = FALSE}

m <- lmer(y ~ x + (1|year) + (1|year:id), data = d)

```

## Quand utiliser les effets aléatoires?

<br>

- Mesures répétées sur certaines unités ou individus

- Les observations sont regroupées sous des structures ou ne sont pas indépendantes

- Il y a de la réplication à plusieurs niveaux de la hiérarchie

- On veut estimer la variance ou généraliser à une population d'unités d'échantillonnage



## Formulation matricielle

<br><br>
<!-- $$ \mu_{ij} = \alpha_{i} + \beta x_{ij}$$

$$ \alpha_{i} \sim \text{Normale}(\alpha,\,\sigma_{\alpha}^{2})\ $$

$$ \textbf{y} \sim \text{Normale}(\mu_{ij},\,\sigma_{\epsilon}^{2})\ $$ -->


$$\boldsymbol{y} \sim \text{Normale}(\mathbf{\beta}\mathbf{X}, \mathbf{I}\sigma^2)$$

$$\boldsymbol{y} \sim \text{Normale}(\mathbf{\beta}\mathbf{X}, \mathbf{\Sigma})$$




## Effet spatial, temporel, phylogénétique

<br>
Autocorrélation spatiale, temporelle, phylogénétique, etc...

## Régularisation (_shrinkage_)

<br>
_Complete pooling_
<br><br>
_No pooling_
<br><br>
_Partial pooling_


## Problèmes, questionnements

<br><br>
<p style='text-align: center'>**La GLMM FAQ!**</p>
<br>
<p style='text-align: center'>[https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)</p>


# 3. Vraisemblance et statistiques bayésiennes

## Rappel des probabilités

Quelle est la probabilité d'obtenir 2 fois pile en tirant une pièce de monnaie 2 fois?

### Pile ou face

On doit multiplier la probabilité de chaque événement pour obtenir la probabilité combinée de ces deux événements. Si $P(pile) = 0.5$ (*i.e.* la probabilité d'otenir pile), on a:

<br>
$$P(pile,pile) = P(pile) \times P(pile) = 0.5 \times 0.5 = 0.25$$
<br>



### Avec R

On peut calculer ceci avec R et la fonction `dbinom`.

```{r}
dbinom(0,size=1,p=0.5) * dbinom(0,size=1,p=0.5)
```

<br>
On peut également appliquer la fonction `dbinom` à un vecteur de valeur. Ceci nous donne un vecteur de (densité de) probabilité et on peut multiplier ces valeurs entre elles avec la fonction `prod`.

```{r}
prod(dbinom(c(0,0),size=1,p=0.5))
```


## Vraisemblance

La vraisemblance (*likelihood*) représente la plausibilité de valeurs de paramètres en fonction de données observées. L'utlisation de la vraisemblance implique l'utilisation de distribution de probablités pour représenter le lien entre les observations et les modèles.
<br><br>
Pour quantifier la vraisemblance, on peut utiliser une fonction de vraisemblance (*likelihood function*).
<br>
$$L(\theta|data) = P(data|\theta)$$
<br>
$data$: données
<br>
$\theta$: paramètres du modèle
<br><br>
$L(\theta|data)$: fonction de vraisemblance
<br>
$P(data|\theta)$: vraisemblance (*likelihood*)

<br>
Ici, $L(\theta|data)$ est la fonction de vraisemblance et l'équation précédente nous dit que la vraisemblance des paramètres conditonnelle aux paramètres $\theta$ est égale à la (densité de) probabilité des données conditionnelle aux paramètres $\theta$.



### Maximum de vraisemblance

L'idée derrière le maximum de vraisemblance (*maximum likelihood*) est de déterminer la valeur des paramètres qui rend l'observation des données la plus probable.


<br><br>
Une grande partie des paramètres que nous estimons sont estimés par la méthode du maximum de vraisemblance. 
<br>
Certaines méthodes plus classique comme la miminimisation de la somme des carrés dans la régression standard sont équivalente à la maximisation de la vraisemblance. La sélection de modèles par AIC repose sur l'utilisation du maximum de vraisemblance. 


### Exemple

Simulons `n = 20` présences/absences avec une probabilité de `p = 0.5`. Ensuite, calculons la vraisemblance d'observer les données générées en fonction de plusieurs valeurs du paramètre p, qui ici est une probabilité.   

<br>
```{r}
set.seed(123)
p<-0.5
y<-rbinom(20,1,0.5)

probs<-seq(0,1,by=0.01)
l<-sapply(probs,function(i){
  prod(dbinom(y,size=1,prob=i)) # likelihood
})
```

<br>
Pour calculer la vraisemblance, on utilise la fonction dbinom qui permet de calculer pour chaque valeur observée une (densité de) probabilité. En calculant le produit `prod` de toutes ces valeurs, on obtient la valeur de la fonction de vraisemblance pour l'observation de l'ensemble de ces données pour une valeur donnée du paramètre `p`. On calcule ensuite ces valeurs de vraisemblance pour un ensemble de valeur de `p`.


### Profil de vraisemblance

On peut par la suite illustrer la vraisemblance d'obtenir les valeurs observées pour les différentes valeurs de `p`. Ceci permet d'illustrer le profile de vraisemblance (*likelihood profile*).

```{r}
plot(probs,l,type="l",ylab="Likelihood",xlab="Valeur du paramètre p")
abline(v=sum(y)/length(y),lty=3)
abline(v=0.5)
legend("topleft",lty=c(1,3),legend=c("Probabilité réelle","Proportion de 1 dans l'échantillon"),bty="n")
probs[which.max(l)]
```

<br>
Le maximum de cette courbe est le maximum de vraisemblance (*maximum likelihood*), i.e. la valeur de `p` pour laquelle la probabilité d'observer les données est la plus grande. Dans ce cas précis, cette valeur est la proportion de 1 dans les données. La plupart des paramètres que nous estimons sont obtenus avec cette méthode.


### Avec un glm

Voici ce qu'on obtient avec un glm si on tente d'estimer cette probabilité. En principe, l'intercept représente cette probabilité, mais elle est sur l'échelle logit $log(p/(1-p))$. Il faut donc retransformer le tout sous l'échelle d'une probabilité avec une fonction logit inverse.

```{r}
m<-glm(y~1,family=binomial)
summary(m)
exp(coef(m))/(1+exp(coef(m)))
```



### Intervalle de confiance 

On peut également estimer un intervalle de confiance autour de cette probabilité avec la fonction `confint.` Différentes méthodes existent pour obtenir un intervallede confiance à partir d'un profil de vraisemblance, mais nous n'arborderons pas cela ici (*i.e.* *profile likelihood confidence intervals*).
<br>
```{r}
ci<-confint(m) # calcul de l'intervalle de confiance sur les coefficients
ci<-c(coef(m),ci) # intercept est l'estimation de la probabilité p
c(exp(ci)/(1+exp(ci))) # intervalle de confiance

```


### Log de la vraisemblance

Pour des raisons pratiques, on va souvent maximiser le *log* de la vraisemblance (*log-likelihood*) pour éviter des problèmes numériques associés aux très petites valeurs. On parle donc souvent de *log-likelihood* (*LL*, *logLik*, etc.).
<br>
Par exemple, on peut comparer la vraisemblance calculée par le glm à celle que nous avons calculée "à la mitaine" avec la fonction `dbinom` pour la valeur du paramètre qui maximise cette vraisemblance (`p = 0.55`).
<br>
```{r}
prod(dbinom(y,size=1,prob=0.55))
exp(logLik(m))
```


## Probabilités conditionnelles

Voyons d'abord ce qu'est une probabilité conditionnelle.
<br>
$$P(A|B)$$
<br>
Ici, on représente la probabilité de $A$ considérant que $B$ s'est produit. Par exemple, quelle est la probabilité d'être infecté par la Covid-19 si on reçoit un test positif.
<br><br>
$$P(\textrm{infecté}|\textrm{test positif})$$
<br><br>
On pourrait aussi s'intéresser à la probabilité d'avoir un test positif si on est effectivement infecté.
$$P(\textrm{test positif}|\textrm{infecté})$$
<br>


### Règle de Bayes

Il s'agit simplement d'une règle en théorie des probabilités qui permet de relier des probabilités conditionnelles.
<br><br>
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
<br><br>

$P(A)$ veut dire la probbailité de $A$, indépendemment de $B$.
<br><br>
$P(A|B)$ veut dire la probabilité de $A$ sachant $B$. C'est une probabilité conditionnelle.


### Interprétation

Un exemple classique de cette règle est le suivant. Supposons que $P(A)$ désigne la probabilité d'être infecté par la Covid-19 et que $P(B)$ désigne la probabilité de tester positif à un test de Covid-19. Ici, on a aussi $P(A')$ qui est la probabilité de ne pas être infecté.
<br><br>
Si on a:
<br>
$P(A) = 1/500 = 0.002$: proportion de personnes infectées

$P(A') = 1-1/500 = 0.998$: proportion de personnes qui ne sont pas infectées

$P(B|A) = 0.98$: probabilité d'un test positif si une personne est infectée

$P(B|A') = 0.95$: probabilité d'un test négatif si une personne n'est pas infectée

$P(B) = P(B|A)P(A) + P(B|A')P(A')$: il faut additionner les possibilités associées au cas où une personne est infectée et au cas où une personne n'est pas infectée. 

<br>

Quelle est la probabilité d'être infecté si on reçoit un test positif (en assumant qu'on se fait tester pour une raison autre que d'avoir des symptômes) ?
<br>

$$P(A|B) = \frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|A')P(A')} = \frac{0.98 \times 0.002}{0.98 x 0.002 \times (1-0.95) \times 0.998} \approx 0.0378$$

<br>
Dans ce cas fictif, la probabilité d'avoir réellement la Covid-19 si le test est positif n'est que d'environ 3.8 %


## Statistiques bayésiennes

Les statisiques bayésiennes se basent sur cette règle pour faire une **inteprétation probabiliste des paramètres** à partir des données.
<br>
$$P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)}$$
<br><br>
$data$: données
<br>
$\theta$: paramètres du modèle
<br><br>
$P(\theta|data)$: distribution postérieure (*posterior*)
<br>
$P(data|\theta)$: vraisemblance (*likelihood*)
<br>
$P(\theta)$: distribution *a priori* (*prior*) 
<br>
$P(data)$: distribution marginale des données (*marginal likelihood*)


### Distribution postérieure (*posterior*)

$$P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)}$$
<br><br>
La distribution postérieure $P(\theta|data)$ est ce que l'on cherche à estimer. Il s'agit d'une distribution de probabilité qui décrit les probabilités des différentes valeurs possibles du paramètre. Contrairement avec les analyses fréquentistes, on peut directement parler de la probabilité qu'un paramètre soit entre telle et telle valeur.

### Vraisemblance (*likelihood*)

$$P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)}$$
<br><br>
Comme vu précédemment, la vraisemblance $P(data|\theta)$ représente la probabilité des données conditionnelles à des valeurs de paramètres $\theta$.

### Distribution *a priori* (*priors*)

$$P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)}$$
<br><br>
Les distributions *a priori* $P(data)$ représentent nos croyances sur les valeurs des paramètres avant de faire les analyses. On les représente à l'aide de distributions de probabilité. Ces croyances représentent ce à quoi on s'attend en termes de valeurs en se basant sur nos connaissances théoriques avant de faire notre expérience.


### Distribution marginale des données

$$P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)} = \frac{P(data|\theta)P(\theta)}{\int P(data|\theta)P(\theta)d\theta}$$
<br><br>
Cette quantité $P(data)$ (*marginal likelihood*) est beaucoup plus difficile à comprendre intuitivement. Elle représente en quelque sorte la probabilité des données pour l'ensemble des valeurs de paramètres possibles (d'où l'intégrale). C'est la distribution marginale des données ou la vraisemblance marginale. Elle permet également de faire en sorte que la distribution postérieure soit une distribution de probabilité (i.e. aire sous la courbe = 1). 
<br><br>
Il s'agit d'une constante qui est généralement très difficile à calculer. Des méthodes comme le MCMC permettent d'éviter de la calculer dans l'obtention de la distribution postérieure.  

### En résumé

$$Posterior = \frac{Likelihood \times Prior}{Marginal\;Likelihood}$$

<br><br>

Puisque le dénominateur est une constante et que dans bien des cas, on peut éviter de le calculer, on peut plus simplement écrire:

<br><br>

$$Posterior \propto Likelihood \times Prior$$

<br>
En d'autres mots, la distribution postérieure est proportionnelle à la vraisemblance et aux distributions *a priori*

<br><br>
On pourrait aussi dire qu'avec une approche bayésienne, on met à jour nos croyances à l'aide de données.


## Avantages

- Interprétation probabiliste plus facile (e.g. intervalle de confiance fréquentiste vs. bayésien)

- Facilite l'estimation de modèles complexes (e.g. modèle hiérarchique)

- Intégration plus facile de l'incertitude à toutes les étapes du processus

- Facilite l'estimation de quantités dérivées (e.g. prédictions ou combinaisons de prédictions)

- Le fait d'avoir à spécifier des *priors* nous force à comprendre ce que nos paramètres représentent
 
- Façon de penser plus naturelle, philosophiquement plus satisfaisante? 


## Désavantages ?

- Souvent plus complexe (mais pas toujours)

- Modèles plus longs à estimer

- Moins de solutions clés en main?

- Nécessité de choisir et de justifier ses *priors*


## Exemple

Simulons d'abord un modèle avec des paramètres connus.
<br>
```{r}
set.seed(1234)
n<-100
x<-runif(n,0,10)
y<-10+2*x+rnorm(n,0,10)
d<-data.frame(y,x)
```

### Quels *priors* utiliser?

Il est toujours préférable de visualiser les *priors* et de réfléchir à ce qu'ils représentent.
<br>
```{r,fig.height=3,fig.width=9}

par(mfrow=c(1,3))
curve(dnorm(x,0,50),from=-200,to=200,main="Intercept normal(0,50)",ylab="Density")
curve(dnorm(x,0,5),from=-20,to=20,main="Pente normal(0,5)",ylab="Density")
curve(dgamma(x,2,0.1),from=0,to=100,main="Écart-type gamma(2,0.1)",ylab="Density")

```

<br>
Par exemple, pour la variance/écart-type du modèle, il faut idéalement un *prior* avec des valeurs strictement positives, car les valeurs négatives ne font pas de sens pour la variance. Une des possibilités est d'utiliser la distribution gamma.


### Modèle avec brms

La façon la plus "clé en main" de faire un modèle bayésien est d'utiliser le package [brms](https://CRAN.R-project.org/package=brms). Ce package permet de spécifier des modèles d'une façon très similaire aux packages fréquemment utilisés comme nlme, lme4, glmmTMB, etc. En arrière-plan, brms utilise le programme [STAN](https://mc-stan.org/) qui est un environnement de programmation pour la modélisation bayésienne.
<br>

```{r, message = FALSE, warning = FALSE, tidy = FALSE, results = 'hide'}
library(brms)

m <- brm(y ~ x,
       data = d,
       cores = 3,
       chains = 3,
       family = "gaussian",
       prior = c(
         set_prior("normal(0,5)", class = "b", coef="x"),
         set_prior("normal(0,50)", class = "Intercept"),
         set_prior("gamma(2,0.1)", class = "sigma")
       )
     )

```


### Visualisation
```{r, message = FALSE,tidy = FALSE}

library(ggeffects)
g <- ggpredict(m, terms = c("x"))
plot(g, add.data = TRUE)

```

### Distribution postérieure des prédictions
```{r, message = FALSE,tidy = FALSE}

newdata <- data.frame(x = seq(min(d$x), max(d$x), length.out = 100))

pp <- posterior_epred(m, newdata, ndraws = 500)

plot(d$x, d$y, ylim = range(c(pp, d$y)), pch = 16, col = gray(0, 0.35)) 
lapply(1:nrow(pp), function(i){
  lines(newdata$x, pp[i, ], col = adjustcolor("black", 0.05))
}) |> invisible()

```





### Formulation du modèle

On a la composante aléatoire, la composante systématique et les *priors*.

<br>
$$y \sim \mathcal{N}(\mu,\sigma^2)$$
$$\mu = \beta_{0}+\beta_{1}x$$
$$\beta_{0} \sim \mathcal{N}(0,50)$$
$$\beta_{1} \sim \mathcal{N}(0,5)$$
$$\sigma \sim \mathcal{Gamma}(2,0.1)$$



### Sommaire

```{r,fig.height=9,fig.width=6}
summary(m)
```
<br>
Notez encore une fois ici qu'on peut faire une interprétation probabiliste plus naturelle des intervalles de confiance. Contrairement aux analyses fréquentistes, on peut dire que le probabilité que la valeur d'un paramètre soit entre les bornes inférieures et supérieures d'un intervalle de confiance à 95% est de 95%. En fait, avec une approche bayésienne, on parle plutôt d'intervalle de crédibilité (*credible interval*).

### Visualisation des paramètres

```{r,fig.height=8,fig.width=9}
plot(m)
```

Les distributions postérieures des paramètres sont illustrées à gauche. À droite, les chaînes de MCMC sont illustrées (nous y reviendrons). <!--Avec les modèles bayésiens, il faut toujours vérifier la convergence des chaînes.-->


## MCMC

Le MCMC (*Markov Chain Monte Carlo*) est une classe d'algorithmes permettant d'échantillonner à partir d'une distribution de probabilité. Il s'agit d'une approche qui permet d'éviter des calculs complexes, voire impossible, en utilisant une approche d'échantillonnage,
<br><br>
Dans un contexte bayésien, le MCMC permet d'échantillonner à partir d'une distribution postérieure. Autrement dit, la distribution postérieure de chaque paramètre estimé dans un contexte bayésien peut entre autre être obtenue par cette méthode d'échantillonnage.
<br><br>
Sous certaines conditions, les valeurs échantillonnées représenteront la distribution postérieure de chaque paramètre. 

### Échantillonnage des chaînes

L'échantillonnage par MCMC se fait de façon séquentielle, *i.e.* que les valeurs seront échantillonnées les unes à la suite des autres. Dans bien des cas, les valeurs actuelles seront dépendentes (corrélées) des valeurs précédentes. On peut parler ici de chaînes pour désigner l'ensemble des valeurs échantillonnées de façon séquentielle.
<br><br>
Pour démarrer chaque chaîne, il faut d'abord sélectionner une valeur de départ pour chaque paramètre estimé. Cette valeur de départ peut potentiellement être très loin d'une valeur probable.
<br><br>
Éventuellement, les chaînes vont s'éloigner de ces valeurs de départ pour converger vers les distributions postérieures. À ce moment, on dira que l'échantillonnage se fait véritablement à partir des distributions postérieures.
<br>
```{r,fig.height=4,fig.width=4.5}
plot(m)
```

### Convergence des chaînes

Avec les analyses bayésiennes où les paramètres sont estimés par MCMC, il faut donc vérifier la convergence des chaînes pour s'assurer que les valeurs échantillonnées représentent bien les distributions postérieures. 
<br><br>
On élimine souvent une certaine portion du début des chaînes pour enlever les valeurs échantillonnées, alors que les chaînes n'ont pas convergées. On parle souvent de *burnin* pous désigner cette partie des chaînes.
<br><br>
Il est également très fréquent d'utiliser plusieurs chaînes (3-4) pour un même paramètre ce qui permet de s'assurer que les chaînes ont toutes convergées en vérifiant qu'elles ont toutes convergées au même endroit. En général, on choisira des valeurs de départ différentes pour chacune des chaînes.
<br><br>
On peut vérifier la convergence des chaînes avec entre autres le `Rhat` (*potential scale reduction statistic* ou *Gelman-Rubin statistic*). Intuitivement, pour un paramètre donné, cette valeur compare la variance inter-chaîne à la variance intra-chaîne pour évaluer si les chaînes ont convergé au même endroit. Si c'est le cas, le `Rhat` sera très près de 1 (`Rhat` <= 1.01)
<br>
```{r}
summary(m)
```

## Priors

L'utilisation de *priors* est souvent vue comme étant la partie la plus controversée des statistiques bayésiennes. Qu'en pensez-vous?
<br><br>
Idéalement, il faut toujours avoir une petite idée de l'influence des *priors* sur l'analyses. Pour cela il est bien de comparer les distributions postérieures aux *priors*. On peut également faire dans certains cas des analyses de sensibilité pour quantifier cette influence.
<br><br>
Plus on a de données, moins les *priors* auront d'influence sur l'estimation des paramètres.

### Types de *priors*

- Informatif: basé sur la théorie, reflète les connaissances.

- Non-informatif: reflète l'absence de connaissances ou le désir de ne pas influencer les résultats.

- Plat (*flat*): similaire à non-informatif.

- Légèrement informatif (*weaky informative*): la théorie n'est pas assez développée pour utiliser des *priors* informatifs, mais assez pour savoir que certaines valeurs ne sont pas probables.

- Par défaut: valeurs par défaut utilisées par les logiciels ou les packages.

- Régularisant (*regularizing*): permet de circonscrire les valeurs obtenues et de pénaliser les valeurs élevées dans un contexte prédictif.

- Etc.

<br>

Voir notamment [Lemoine 2019](https://doi.org/10.1111/oik.05985) et [Banner et al. 2020](https://doi.org/10.1111/2041-210X.13407) pour une discussion sur l'utilisation des priors en écologie.


### Quels *priors* utiliser?

Il faut aussi sélectionner une distribution approriée pour représenter nos *priors* et les distributions utilisées doivent être appropriées pour un paramètre donnée. Le groupe qui travaille sur la plateforme [STAN](https://mc-stan.org/) a plusieurs recommandations de *priors* à utiliser dépendemment du contexte (*priors* pour des pentes, *priors* pour des variances, *priors* pour des variances dans un contexte de modèles hiérarchiques, etc.)
<br>

Voir [Prior Choice Recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) pour des recommandations.


### Attention aux *priors* "non-informatifs"!

Les *priors* non-informatifs ne le sont pas toujours sous l'échelle du paramètre. Par exemple, si vous tentez d'estimer une probabilité et que vous utiliser une distribution normale comme *prior* pour l'intercept du modèle, il se peut que ce *prior* soit très informatif sous l'échelle du prédicteur linéaire qui est relié à la probabilité par une fonction logit. Supposons un glm binomial avec seulement un intercept.
<br><br>
$$ y \sim \mathcal{Binom}(1,p) = \mathcal{Bernoulli}(p)$$
<br>
$$ logit(p) = log\left(\frac{p}{1-p}\right) = \beta_{0} $$
<br>
$$ \beta_{0} \sim \mathcal{N}(0,10) $$
<br>

### Attention aux *priors* "non-informatifs"!
```{r,fig.height=4,fig.width=8,}
n<-10000
x<-rnorm(n,0,10)
par(mfrow=c(1,2))
hist(x,breaks=50,main="Valeurs a priori de l'intercept")
hist(exp(x)/(1+exp(x)),breaks=50,main="Lorsque converties en probabilité")

```
<br>
Autrement dit, le *prior* est très informatif et pousse les valeurs vers des probabilitées extrêmes!


### Étudier ses priors

*prior predictive distribution*

Permet de visualiser l'effet des priors sur l'échelle de la réponse.
```{r, message = FALSE,tidy = FALSE, warning = FALSE, results = 'hide'}

m2 <- update(m, sample_prior = "only")

```


```{r, message = FALSE,tidy = FALSE, warning = FALSE}
newdata <- data.frame(x = seq(min(d$x), max(d$x), length.out = 100))

pp <- posterior_epred(m2, newdata, ndraws = 500)

plot(d$x, d$y, ylim = range(c(pp, d$y)), pch = 16, col = gray(0, 0.35)) 
lapply(1:nrow(pp), function(i){
  lines(newdata$x, pp[i, ], col = adjustcolor("black", 0.05))
}) |> invisible()

```

#### Avec un *prior* aberrant
```{r, message = FALSE,tidy = FALSE, results = 'hide'}

newdata <- data.frame(x = seq(min(d$x), max(d$x), length.out = 100))

m3 <- update(m, 
        sample_prior = "only",
        prior = c(
           set_prior("normal(50,20)", class = "b", coef="x"),
           set_prior("normal(0,50)", class = "Intercept"),
           set_prior("gamma(2,0.1)", class = "sigma")
        ),
      )
```


```{r, message = FALSE,tidy = FALSE}
pp <- posterior_epred(m3, newdata, ndraws = 500)

plot(d$x, d$y, ylim = range(c(pp, d$y)), pch = 16, col = gray(0, 0.35)) 
lapply(1:nrow(pp), function(i){
  lines(newdata$x, pp[i, ], col = adjustcolor("black", 0.05))
}) |> invisible()

```

#### Avec un modèle binomial
```{r, message = FALSE,tidy = FALSE, results = 'hide'}

logit <- function(p) {
  log(p / (1 - p))
}

inv.logit <- function(x) {
   exp(x) / (1 + exp(x))  
}

n <- 200
x <- runif(n, 0, 10)
z <- -2 + 0.5 * x
y <- rbinom(n, size = 1, prob = inv.logit(z))

d<-data.frame(y,x)

m <- brm(y ~ x,
       data = d,
       cores = 3,
       chains = 3,
       family = "bernoulli",
       prior = c(
         set_prior("normal(0,5)", class = "b", coef="x"),
         set_prior("normal(0,5)", class = "Intercept")
       )
     )
```


#### Distribution postérieure des prédictions
```{r, message = FALSE,tidy = FALSE}
newdata <- data.frame(x = seq(min(d$x), max(d$x), length.out = 100))

pp <- posterior_epred(m, newdata, ndraws = 500)

plot(d$x, d$y, ylim = range(c(pp, d$y)), pch = 16, col = gray(0, 0.35)) 
lapply(1:nrow(pp), function(i){
  lines(newdata$x, pp[i, ], col = adjustcolor("black", 0.05))
}) |> invisible()


```


#### Ce qu'indique notre *prior*
```{r, message = FALSE, tidy = FALSE, results = 'hide'}

m2 <- update(m, sample_prior = "only")
```


```{r, message = FALSE, tidy = FALSE}
pp <- posterior_epred(m2, newdata, ndraws = 500)

plot(d$x, d$y, ylim = range(c(pp, d$y)), pch = 16, col = gray(0, 0.35)) 
lapply(1:nrow(pp), function(i){
  lines(newdata$x, pp[i, ], col = adjustcolor("black", 0.05))
}) |> invisible()
```


#### Prédiction pour `x = 2`
```{r, message = FALSE,tidy = FALSE}
pp <- posterior_epred(m2, data.frame(x = 2), ndraws = 500)
hist(pp, breaks = 50)
```


### Un *prior* plus raisonnable?
```{r, message = FALSE,tidy = FALSE, results = 'hide'}

m3 <- update(m,
       sample_prior = "only",       
       prior = c(
         set_prior("normal(0,1)", class = "b", coef="x"),
         set_prior("normal(0,1)", class = "Intercept")
       )
     )
```


```{r, message = FALSE, tidy = FALSE}
pp <- posterior_epred(m3, newdata, ndraws = 500)

plot(d$x, d$y, ylim = range(c(pp, d$y)), pch = 16, col = gray(0, 0.35)) 
lapply(1:nrow(pp), function(i){
  lines(newdata$x, pp[i, ], col = adjustcolor("black", 0.05))
}) |> invisible()
```


## Recommandations sur les priors

[**Prior Choice Recommendations**](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations) Recommandation par les développeurs de STAN

[**Moving beyond noninformative priors: why and how to choose weakly informative priors in Bayesian analyses**](https://doi.org/10.1111/oik.05985)

[**The use of Bayesian priors in Ecology: The good, the bad and the not great**]( https://doi.org/10.1111/2041-210X.13407)

[**Choosing priors in Bayesian ecological models by simulating from the prior predictive distribution**]( https://doi.org/10.1002/ecs2.3739)


## Outils

Il existe plusieurs outils pour faire des modèles bayésiens.
<br><br>

[**WinBUGS**](https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-winbugs/) Un des premiers outils, mais un peu lent
<br><br>
[**JAGS**](https://sourceforge.net/projects/mcmc-jags/files/Manuals/) Similaire à BUGS, mais un peu plus rapide
<br><br>
[**STAN**](https://mc-stan.org/) Rapide et bien documenté
<br><br>
[**Nimble**](https://r-nimble.org/) Relativement nouveau
<br><br>
[**MCMCglmm**](https://CRAN.R-project.org/package=MCMCglmm) Modèle animal?
<br><br>
[**brms**](https://CRAN.R-project.org/package=brms) Facile d'utilisation et basé sur STAN
<br><br>
[**INLA**](https://www.r-inla.org/) Modèles spatiaux




# Liens utiles

[A protocol for data exploration to avoid common statistical problems](https://doi.org/10.1111/j.2041-210X.2009.00001.x)

[A protocol for conducting and presenting results of regression-type analyses](https://doi.org/10.1111/2041-210X.12577)

[Applied statistics in ecology: common pitfalls and simple solutions](https://doi.org/10.1890/ES13-00160.1)

[Simple means to improve the interpretability of regression coefficients](https://doi.org/10.1111/j.2041-210X.2010.00012.x)

<!-- to do, modèle linéaire, simul t.test, simule CI, check début, each verif assump plot, à compléter, revise all, list Zuur papers, read Marc again -->
